{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import glob\n",
    "import ast "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è³‡æ–™è¼‰å…¥é¸é … ===\n",
      "ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: False\n",
      "ä½¿ç”¨ä¸²æµæ¨¡å¼: False\n",
      "ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: True\n",
      "\n",
      "â¬‡ï¸ ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†...\n",
      "è­¦å‘Šï¼šé€™å°‡ä¸‹è¼‰ 13.7GB çš„è³‡æ–™ï¼Œå¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed5eb895244fd7bb9f4ef1ab596ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# è³‡æ–™è®€å–é¸é …\n",
    "# æ‚¨å¯ä»¥é¸æ“‡ä»¥ä¸‹ä»»ä¸€ç¨®æ–¹å¼ä¾†è¼‰å…¥è³‡æ–™ï¼š\n",
    "\n",
    "# é¸é … 1: å¾å·²å„²å­˜çš„æœ¬åœ°æª”æ¡ˆè®€å– (æ¨è–¦ï¼Œé€Ÿåº¦å¿«)\n",
    "use_local_files = False\n",
    "\n",
    "# é¸é … 2: å¾ Hugging Face ç›´æ¥ä¸²æµè¼‰å…¥ (éœ€è¦ç¶²è·¯é€£ç·š)\n",
    "use_streaming = False\n",
    "\n",
    "# é¸é … 3: ä¸‹è¼‰å®Œæ•´è³‡æ–™é›† (æª”æ¡ˆå¾ˆå¤§ï¼Œä¸æ¨è–¦)\n",
    "use_full_download = True\n",
    "\n",
    "print(\"=== è³‡æ–™è¼‰å…¥é¸é … ===\")\n",
    "print(f\"ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: {use_local_files}\")\n",
    "print(f\"ä½¿ç”¨ä¸²æµæ¨¡å¼: {use_streaming}\")\n",
    "print(f\"ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: {use_full_download}\")\n",
    "\n",
    "# è³‡æ–™è¼‰å…¥\n",
    "if use_local_files:\n",
    "    print(\"\\nğŸ“ å¾æœ¬åœ°æª”æ¡ˆè®€å–è³‡æ–™...\")\n",
    "    \n",
    "    # æª¢æŸ¥å·²å„²å­˜çš„æª”æ¡ˆ\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # å°‹æ‰¾å¯ç”¨çš„æª”æ¡ˆ\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"æ‰¾åˆ°çš„æª”æ¡ˆ:\")\n",
    "        print(f\"  CSV æª”æ¡ˆ: {len(csv_files)} å€‹\")\n",
    "        print(f\"  JSON æª”æ¡ˆ: {len(json_files)} å€‹\")\n",
    "        print(f\"  Parquet æª”æ¡ˆ: {len(parquet_files)} å€‹\")\n",
    "        \n",
    "        # å„ªå…ˆä½¿ç”¨ Parquet æª”æ¡ˆ (æœ€é«˜æ•ˆ)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ Parquet æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # å…¶æ¬¡ä½¿ç”¨ CSV æª”æ¡ˆ\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ CSV æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # æœ€å¾Œä½¿ç”¨ JSON æª”æ¡ˆ\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ JSON æª”æ¡ˆ: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ²’æœ‰æ‰¾åˆ°å·²å„²å­˜çš„è³‡æ–™æª”æ¡ˆ\")\n",
    "            print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"âŒ æ‰¾ä¸åˆ° saved_datasets ç›®éŒ„\")\n",
    "        print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\nğŸŒ å¾ Hugging Face ä¸²æµè¼‰å…¥è³‡æ–™...\")\n",
    "    \n",
    "    # ä½¿ç”¨ä¸²æµæ¨¡å¼è¼‰å…¥è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # è¨­å®šè¦è¼‰å…¥çš„æ¨£æœ¬æ•¸é‡ - æ¸›å°‘åˆ°100ç­†ç”¨æ–¼æ¼”ç¤º\n",
    "    num_samples = 1000\n",
    "    print(f\"è¼‰å…¥å‰ {num_samples} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # æ”¶é›†è³‡æ–™\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  å·²è¼‰å…¥ {i + 1} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\nâ¬‡ï¸ ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†...\")\n",
    "    print(\"è­¦å‘Šï¼šé€™å°‡ä¸‹è¼‰ 13.7GB çš„è³‡æ–™ï¼Œå¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“\")\n",
    "    \n",
    "    # ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰é¸æ“‡ä»»ä½•è³‡æ–™è¼‰å…¥é¸é …\")\n",
    "    df = None\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™è³‡è¨Š\n",
    "if df is not None:\n",
    "    print(f\"\\nâœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\")\n",
    "    print(f\"ğŸ“Š è³‡æ–™å½¢ç‹€: {df.shape}\")\n",
    "    print(f\"ğŸ“‹ æ¬„ä½åç¨±: {list(df.columns)}\")\n",
    "    \n",
    "    # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\nğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # é¡¯ç¤ºå‰100å€‹å­—ç¬¦\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"ç¯„ä¾‹ {i+1} ({len(text)} å­—ç¬¦): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ è³‡æ–™å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è©•åˆ†è™•ç†ï¼\")\n",
    "else:\n",
    "    print(\"\\nâŒ è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®šä¸¦é‡æ–°åŸ·è¡Œ\")\n",
    "\n",
    "# å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸ä¾›å¾ŒçºŒä½¿ç”¨\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = \"saved_datasets\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(save_dir, \"test_cluecorpus.csv\"), index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# dataset_df = pd.read_csv(\"test_cluecorpus.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc60868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\n",
    "    \"test_cluecorpus.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    nrows=1000   # åªè®€å–å‰ 1000 ç­†\n",
    ")\n",
    "\n",
    "print(len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633432",
   "metadata": {},
   "source": [
    "## ğŸ“ æ–‡æœ¬åˆ‡åˆ†è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c683f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”ª å•Ÿå‹•æ–‡æœ¬åˆ‡åˆ†è™•ç† (å«æ–·é»çºŒå­˜)...\n",
      "\n",
      "âš™ï¸ åˆ‡åˆ†åƒæ•¸:\n",
      "  æœ€å°ç‰‡æ®µé•·åº¦: 30 å­—\n",
      "  æœ€å¤§ç‰‡æ®µé•·åº¦: 250 å­—\n",
      "  åˆ‡åˆ†æ¨¡å¼: å¥å­ç´šåˆ¥ + æ–·é»çºŒå­˜\n",
      "ğŸ“Š é–‹å§‹è™•ç†æ–‡æœ¬åˆ‡åˆ†...\n",
      "  åŸå§‹è³‡æ–™ç­†æ•¸: 1000\n",
      "  å¥å­ç‰‡æ®µé•·åº¦ç¯„åœ: 30-250 å­—\n",
      "ğŸ” å·²è®€å–éƒ¨åˆ†è™•ç†çµæœï¼š32800 ç­†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åˆ‡åˆ†é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:02<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆï¼Œå·²åˆªé™¤æ–·é»æª”\n",
      "\n",
      "âœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼\n",
      "  è™•ç†æ–‡æœ¬æ•¸: 794\n",
      "  ç”Ÿæˆå¥å­ç‰‡æ®µæ•¸: 36670\n",
      "  å¹³å‡æ¯æ–‡æœ¬ç‰‡æ®µæ•¸: 46.2\n",
      "\n",
      "ğŸ“ ç‰‡æ®µé•·åº¦çµ±è¨ˆ:\n",
      "  å¹³å‡é•·åº¦: 62.0 å­—\n",
      "  æœ€çŸ­ç‰‡æ®µ: 30 å­—\n",
      "  æœ€é•·ç‰‡æ®µ: 1049 å­—\n",
      "  ä¸­ä½æ•¸é•·åº¦: 51.0 å­—\n",
      "\n",
      "ğŸ’¾ å®Œæˆè¼¸å‡º:\n",
      "  ğŸ“„ CSV: split_datasets/sentence_fragments_20250911_201937.csv\n",
      "  ğŸ“‹ JSON: split_datasets/sentence_fragments_20250911_201937.json\n",
      "  ğŸ“¦ Parquet: split_datasets/sentence_fragments_20250911_201937.parquet\n",
      "\n",
      "ğŸ¯ è®Šæ•¸åç¨±: split_dataset_df\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"ğŸ”ª å•Ÿå‹•æ–‡æœ¬åˆ‡åˆ†è™•ç† (å«æ–·é»çºŒå­˜)...\")\n",
    "\n",
    "# =========================================================\n",
    "# å¥å­åˆ‡åˆ†æ ¸å¿ƒå‡½æ•¸\n",
    "# =========================================================\n",
    "def split_text_to_sentences(text, min_length=30, max_length=250):\n",
    "    sentence_separators = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'â€¦']  # å¼·åˆ†éš”\n",
    "    phrase_separators = ['ï¼Œ', 'ã€', 'ï¼š', 'ï¼›']       # å¼±åˆ†éš”\n",
    "\n",
    "    sentences, current_sentence = [], \"\"\n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in sentence_separators:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "\n",
    "    fragments = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) < min_length:\n",
    "            continue\n",
    "        if len(sentence) <= max_length:\n",
    "            fragments.append(sentence)\n",
    "        else:\n",
    "            # é•·å¥å†ç´°åˆ‡\n",
    "            parts, current_part = [], \"\"\n",
    "            for char in sentence:\n",
    "                current_part += char\n",
    "                if char in phrase_separators:\n",
    "                    if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                        parts.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "            if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                parts.append(current_part.strip())\n",
    "\n",
    "            merged_parts, temp_part = [], \"\"\n",
    "            for part in parts:\n",
    "                if len(temp_part + part) <= max_length:\n",
    "                    temp_part = temp_part + part if temp_part else part\n",
    "                else:\n",
    "                    if temp_part and len(temp_part) >= min_length:\n",
    "                        merged_parts.append(temp_part)\n",
    "                    temp_part = part\n",
    "            if temp_part and len(temp_part) >= min_length:\n",
    "                merged_parts.append(temp_part)\n",
    "\n",
    "            # ç„¡æ³•ç´°åˆ‡å°±ç¡¬åˆ‡\n",
    "            if not merged_parts and len(sentence) >= min_length:\n",
    "                for i in range(0, len(sentence), max_length):\n",
    "                    fragment = sentence[i:i + max_length]\n",
    "                    if len(fragment) >= min_length:\n",
    "                        merged_parts.append(fragment)\n",
    "\n",
    "            fragments.extend(merged_parts)\n",
    "    return fragments\n",
    "\n",
    "def split_text_by_punctuation(text, min_length=30, max_length=250):\n",
    "    return split_text_to_sentences(text, min_length, max_length)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# æ–·é»çºŒå­˜è¼”åŠ©å‡½æ•¸\n",
    "# =========================================================\n",
    "CHECKPOINT_FILE = \"split_checkpoint.json\"\n",
    "PARTIAL_FILE    = \"split_partial.csv\"\n",
    "\n",
    "def save_checkpoint(last_index):\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": last_index}, f)\n",
    "    # print(f\"ğŸ’¾ å·²å„²å­˜é€²åº¦ï¼šindex {last_index}\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            ckpt = json.load(f)\n",
    "        print(f\"ğŸ”„ åµæ¸¬åˆ°é€²åº¦æª”ï¼Œå°‡å¾ index {ckpt['last_index'] + 1} ç¹¼çºŒ\")\n",
    "        return ckpt[\"last_index\"]\n",
    "    return -1\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ä¸»è™•ç†æµç¨‹ï¼ˆå·²åŠ å…¥æ–·é»çºŒå­˜ï¼‰\n",
    "# =========================================================\n",
    "def process_text_splitting(df, text_column='text', min_length=30, max_length=250):\n",
    "    print(f\"ğŸ“Š é–‹å§‹è™•ç†æ–‡æœ¬åˆ‡åˆ†...\")\n",
    "    print(f\"  åŸå§‹è³‡æ–™ç­†æ•¸: {len(df)}\")\n",
    "    print(f\"  å¥å­ç‰‡æ®µé•·åº¦ç¯„åœ: {min_length}-{max_length} å­—\")\n",
    "\n",
    "    last_index = load_checkpoint()\n",
    "    split_data = []\n",
    "\n",
    "    # è®€å–éƒ¨åˆ†è¼¸å‡ºä»¥ä¾¿çºŒå­˜\n",
    "    if os.path.exists(PARTIAL_FILE):\n",
    "        split_data = pd.read_csv(PARTIAL_FILE).to_dict(\"records\")\n",
    "        print(f\"ğŸ” å·²è®€å–éƒ¨åˆ†è™•ç†çµæœï¼š{len(split_data)} ç­†\")\n",
    "\n",
    "    total_fragments, processed_texts = len(split_data), 0\n",
    "\n",
    "    CHECKPOINT_INTERVAL = 1000\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"åˆ‡åˆ†é€²åº¦\"):\n",
    "        if idx <= last_index:\n",
    "            continue  # è·³éå·²è™•ç†\n",
    "\n",
    "        if idx % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(idx)\n",
    "            pd.DataFrame(split_data).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        original_text = row[text_column]\n",
    "        if pd.isna(original_text):\n",
    "            continue          # â¬…ï¸ å»ºè­°ç›´æ¥è·³é\n",
    "        original_text = str(original_text)\n",
    "        \n",
    "        if len(original_text) < min_length:\n",
    "            continue\n",
    "\n",
    "        search_start_pos = 0\n",
    "        fragments = split_text_to_sentences(original_text, min_length, max_length)\n",
    "\n",
    "        for frag_idx, fragment in enumerate(fragments):\n",
    "            start_pos = original_text.find(fragment, search_start_pos)\n",
    "            end_pos   = start_pos + len(fragment) if start_pos != -1 else -1\n",
    "            if start_pos != -1:\n",
    "                search_start_pos = end_pos\n",
    "\n",
    "            new_row = row.copy()\n",
    "            new_row[text_column] = fragment\n",
    "            new_row['original_index'] = idx\n",
    "            new_row['fragment_index'] = frag_idx\n",
    "            new_row['original_text_length'] = len(original_text)\n",
    "            new_row['fragment_length'] = len(fragment)\n",
    "            new_row['source_type'] = 'sentence_fragment'\n",
    "            new_row['fragment_start'] = start_pos\n",
    "            new_row['fragment_end'] = end_pos\n",
    "            split_data.append(new_row)\n",
    "            total_fragments += 1\n",
    "\n",
    "        processed_texts += 1\n",
    "\n",
    "        # âœ… æ¯è™•ç†ä¸€ç­†å°±å­˜æª”\n",
    "        save_checkpoint(idx)\n",
    "        pd.DataFrame(split_data).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # å…¨éƒ¨å®Œæˆå¾Œåˆªé™¤ checkpoint\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "        print(\"ğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆï¼Œå·²åˆªé™¤æ–·é»æª”\")\n",
    "\n",
    "    split_df = pd.DataFrame(split_data)\n",
    "\n",
    "    print(f\"\\nâœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼\")\n",
    "    print(f\"  è™•ç†æ–‡æœ¬æ•¸: {processed_texts}\")\n",
    "    print(f\"  ç”Ÿæˆå¥å­ç‰‡æ®µæ•¸: {total_fragments}\")\n",
    "    if processed_texts:\n",
    "        print(f\"  å¹³å‡æ¯æ–‡æœ¬ç‰‡æ®µæ•¸: {total_fragments/processed_texts:.1f}\")\n",
    "\n",
    "    if not split_df.empty:\n",
    "        length_stats = split_df['fragment_length'].describe()\n",
    "        print(f\"\\nğŸ“ ç‰‡æ®µé•·åº¦çµ±è¨ˆ:\")\n",
    "        print(f\"  å¹³å‡é•·åº¦: {length_stats['mean']:.1f} å­—\")\n",
    "        print(f\"  æœ€çŸ­ç‰‡æ®µ: {length_stats['min']:.0f} å­—\")\n",
    "        print(f\"  æœ€é•·ç‰‡æ®µ: {length_stats['max']:.0f} å­—\")\n",
    "        print(f\"  ä¸­ä½æ•¸é•·åº¦: {length_stats['50%']:.1f} å­—\")\n",
    "\n",
    "    return split_df\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# åŸ·è¡Œæµç¨‹ç¤ºç¯„\n",
    "# =========================================================\n",
    "if 'dataset_df' in globals() and dataset_df is not None:\n",
    "    MIN_FRAGMENT_LENGTH = 30\n",
    "    MAX_FRAGMENT_LENGTH = 250\n",
    "\n",
    "    print(f\"\\nâš™ï¸ åˆ‡åˆ†åƒæ•¸:\")\n",
    "    print(f\"  æœ€å°ç‰‡æ®µé•·åº¦: {MIN_FRAGMENT_LENGTH} å­—\")\n",
    "    print(f\"  æœ€å¤§ç‰‡æ®µé•·åº¦: {MAX_FRAGMENT_LENGTH} å­—\")\n",
    "    print(f\"  åˆ‡åˆ†æ¨¡å¼: å¥å­ç´šåˆ¥ + æ–·é»çºŒå­˜\")\n",
    "\n",
    "    split_dataset_df = process_text_splitting(\n",
    "        df=dataset_df,\n",
    "        text_column='text',\n",
    "        min_length=MIN_FRAGMENT_LENGTH,\n",
    "        max_length=MAX_FRAGMENT_LENGTH\n",
    "    )\n",
    "\n",
    "    if not split_dataset_df.empty:\n",
    "        # å¦å­˜æœ€çµ‚æª”æ¡ˆï¼ˆé™„æ™‚é–“æˆ³ï¼‰\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        split_dir = \"split_datasets\"\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        base_filename = f\"{split_dir}/sentence_fragments_{timestamp}\"\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "\n",
    "        int_cols = [\n",
    "        \"original_index\", \"fragment_index\",\n",
    "        \"original_text_length\", \"fragment_length\",\n",
    "        \"fragment_start\", \"fragment_end\"\n",
    "        ]\n",
    "        for c in int_cols:\n",
    "            if c in split_dataset_df.columns:\n",
    "                split_dataset_df[c] = (\n",
    "                    pd.to_numeric(split_dataset_df[c], errors=\"coerce\")\n",
    "                    .astype(\"Int64\")\n",
    "                )\n",
    "\n",
    "        split_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        split_dataset_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        split_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "\n",
    "        print(f\"\\nğŸ’¾ å®Œæˆè¼¸å‡º:\")\n",
    "        print(f\"  ğŸ“„ CSV: {csv_filename}\")\n",
    "        print(f\"  ğŸ“‹ JSON: {json_filename}\")\n",
    "        print(f\"  ğŸ“¦ Parquet: {parquet_filename}\")\n",
    "\n",
    "        globals()['split_dataset_df'] = split_dataset_df\n",
    "        print(f\"\\nğŸ¯ è®Šæ•¸åç¨±: split_dataset_df\")\n",
    "    else:\n",
    "        print(\"âŒ æ²’æœ‰ç”¢ç”Ÿæœ‰æ•ˆç‰‡æ®µ\")\n",
    "        split_dataset_df = None\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°è³‡æ–™é›†ï¼Œè«‹å…ˆåŸ·è¡Œ Get Data\")\n",
    "    split_dataset_df = None\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7ae23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>original_index</th>\n",
       "      <th>fragment_index</th>\n",
       "      <th>original_text_length</th>\n",
       "      <th>fragment_length</th>\n",
       "      <th>source_type</th>\n",
       "      <th>fragment_start</th>\n",
       "      <th>fragment_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>å…°å·å…¬äº¤é›†å›¢:æ˜èµ·è‡³8æœˆåº•ä¸‰æ¡å…¬äº¤çº¿æš‚ä¸ç»è¿‡é–è¿œè·¯ç«™åŸæ ‡é¢˜ï¼šæ˜èµ·è‡³8æœˆåº•ä¸‰æ¡å…¬äº¤çº¿æš‚ä¸ç»è¿‡é–...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>155</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>æ®æ‚‰ï¼Œ35è·¯å…¬äº¤çº¿ç”±å…°å·è¥¿å®¢ç«™å¼€å¾€ç¾ä¼¦å¹¿åœºæ—¶ï¼Œå…°å·è¥¿å®¢ç«™è‡³ç™½å¡”å±±å…¬å›­çº¿è·¯ä¸å˜ï¼Œç»åŒ—æ»¨æ²³è·¯ä¹å·...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "      <td>71</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>155</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ç”±ç¾ä¼¦å¹¿åœºå¼€å¾€å…°å·è¥¿å®¢ç«™æ—¶ï¼Œç¾ä¼¦å¹¿åœºè‡³åº™æ»©å­çº¿è·¯ä¸å˜ï¼Œå·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³åŒ—æ»¨æ²³è·¯å¸‚äºŒåŒ»é™¢æ¢...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>545</td>\n",
       "      <td>53</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>226</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81è·¯å…¬äº¤çº¿è·¯ç”±å¸‚äºŒåŒ»é™¢å¼€å¾€çœå†›å¹²æ‰€æ—¶ï¼Œç»åŒ—æ»¨æ²³è·¯ä¹å·å¤§é“å—å£å·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³åº™æ»©å­æ¢å¤...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>545</td>\n",
       "      <td>52</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>279</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ç”±çœå†›å¹²æ‰€å¼€å¾€å¸‚äºŒåŒ»é™¢æ—¶ï¼Œçœå†›å¹²æ‰€è‡³åº™æ»©å­çº¿è·¯ä¸å˜ï¼Œå·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³å¸‚äºŒåŒ»é™¢ã€‚</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>545</td>\n",
       "      <td>41</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>331</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_length  \\\n",
       "0  å…°å·å…¬äº¤é›†å›¢:æ˜èµ·è‡³8æœˆåº•ä¸‰æ¡å…¬äº¤çº¿æš‚ä¸ç»è¿‡é–è¿œè·¯ç«™åŸæ ‡é¢˜ï¼šæ˜èµ·è‡³8æœˆåº•ä¸‰æ¡å…¬äº¤çº¿æš‚ä¸ç»è¿‡é–...          545   \n",
       "1  æ®æ‚‰ï¼Œ35è·¯å…¬äº¤çº¿ç”±å…°å·è¥¿å®¢ç«™å¼€å¾€ç¾ä¼¦å¹¿åœºæ—¶ï¼Œå…°å·è¥¿å®¢ç«™è‡³ç™½å¡”å±±å…¬å›­çº¿è·¯ä¸å˜ï¼Œç»åŒ—æ»¨æ²³è·¯ä¹å·...          545   \n",
       "2  ç”±ç¾ä¼¦å¹¿åœºå¼€å¾€å…°å·è¥¿å®¢ç«™æ—¶ï¼Œç¾ä¼¦å¹¿åœºè‡³åº™æ»©å­çº¿è·¯ä¸å˜ï¼Œå·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³åŒ—æ»¨æ²³è·¯å¸‚äºŒåŒ»é™¢æ¢...          545   \n",
       "3  81è·¯å…¬äº¤çº¿è·¯ç”±å¸‚äºŒåŒ»é™¢å¼€å¾€çœå†›å¹²æ‰€æ—¶ï¼Œç»åŒ—æ»¨æ²³è·¯ä¹å·å¤§é“å—å£å·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³åº™æ»©å­æ¢å¤...          545   \n",
       "4          ç”±çœå†›å¹²æ‰€å¼€å¾€å¸‚äºŒåŒ»é™¢æ—¶ï¼Œçœå†›å¹²æ‰€è‡³åº™æ»©å­çº¿è·¯ä¸å˜ï¼Œå·¦è½¬å¼¯è¿›å…¥ä¹å·å¤§é“è‡³å¸‚äºŒåŒ»é™¢ã€‚          545   \n",
       "\n",
       "   original_index  fragment_index  original_text_length  fragment_length  \\\n",
       "0               3               0                   545              155   \n",
       "1               3               1                   545               71   \n",
       "2               3               2                   545               53   \n",
       "3               3               3                   545               52   \n",
       "4               3               4                   545               41   \n",
       "\n",
       "         source_type  fragment_start  fragment_end  \n",
       "0  sentence_fragment               0           155  \n",
       "1  sentence_fragment             155           226  \n",
       "2  sentence_fragment             226           279  \n",
       "3  sentence_fragment             279           331  \n",
       "4  sentence_fragment             331           372  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " split_dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa141b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== CONFIG ====\n",
    "# MODEL_API_ENDPOINT = \"https://labor-â€¨openwebui.dgx-coolify.apmic.ai/api/\"\n",
    "# API_KEY  = \"è«‹å¡«å…¥è‡ªå·±çš„API KEY\"\n",
    "# MODEL_NAME  = \"gemma-3-27b-it\"\n",
    "# BATCH_SIZE  = 20          # è¦–APIèƒ½åŠ›å¯èª¿\n",
    "# THRESHOLD   = 3\n",
    "# OUTPUT_DIR  = \"./results\" # å„²å­˜çµæœçš„è³‡æ–™å¤¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY =\"sk-b56c488f33b94df297a6314bd037b805\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41859ef8",
   "metadata": {},
   "source": [
    "## gemma- 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35f7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\n",
      "============================================================\n",
      "âœ… ä½¿ç”¨ å¥å­ç‰‡æ®µè³‡æ–™é›†ï¼Œå…± 36670 ç­†è¨˜éŒ„\n",
      "\n",
      "ğŸš€ é–‹å§‹éåŒæ­¥æ‰¹æ¬¡è™•ç†ï¼Œæ¯æ‰¹ 20 ç­†...\n",
      "ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š36670 ç­†\n",
      "ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š36670 ç­†\n",
      "ğŸ”„ åµæ¸¬åˆ°é€²åº¦æª”ï¼Œå°‡å¾ index 35803 ç¹¼çºŒ\n",
      "ğŸ” å·²è®€å–éƒ¨åˆ†è™•ç†çµæœï¼š100 ç­†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éåŒæ­¥æ‰¹æ¬¡æ¨è«–: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1834/1834 [09:24<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ç¯©é¸çµæœçµ±è¨ˆ:\n",
      "  âœ… çœŸæ­£å¤§é™¸ç”¨èª: 109 ç­†\n",
      "  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: 761 ç­†\n",
      "  ğŸ“ˆ ç¯©é¸ç‡: 11.2%\n",
      "\n",
      "ğŸ“ é«˜è³ªé‡å¤§é™¸ç”¨èªç¯„ä¾‹:\n",
      "  1. (å¾—åˆ†:3) ç›®å‰ä»å›½å†…æ¥çœ‹ï¼Œinstagramç•¥èƒœä¸€ç­¹ï¼Œå› ä¸ºå›½å†…èƒ½ä¹°çš„ä¸Šiphoneçš„è°ä¸è‡ªæ‹ï¼Ÿ\n",
      "  2. (å¾—åˆ†:4) é±¼å­é…±å’¸é±¼ä»€ä¹ˆçš„ï¼šå†²ç€é±¼å­é…±ç‚¹çš„ï¼Œ ç»“æœçœŸå¾—ä¸å’‹åœ°ï¼Œå“ï¼Œçœ‹æ¥æˆ‘è¿™è¾ˆå­ä¹Ÿå°±æ˜¯ä¸ªç©·äººçš„å‘½å•¦ã€‚\n",
      "  3. (å¾—åˆ†:3) å°é»„é±¼çƒ§è±†è…ï¼šå¾ˆå«©å¾ˆå«©çš„ï¼Œé±¼ä¹Ÿå«©ï¼Œè±†è…ä¹Ÿå«©ï¼Œå°±æ˜¯ä¸€ç¢°å°±ç¢ï¼Œæœ‰ç‚¹ä¸çŸ¥é“æ€ä¹ˆä¸‹æ‰‹ã€‚\n",
      "\n",
      "ğŸ’¾ å„²å­˜çµæœ...\n",
      "ğŸ’¾ å„²å­˜å®Œæˆ:\n",
      "  ğŸ“„ å®Œæ•´çµæœ: mainland_filtering_complete_20250911_203642.csv\n",
      "  âœ… é«˜è³ªé‡å¥å­ç‰‡æ®µæ•¸æ“š: authentic_mainland_texts_20250911_203642.csv\n",
      "  ğŸ“‹ JSONæ ¼å¼: authentic_mainland_texts_20250911_203642.json\n",
      "\n",
      "ğŸ‰ å¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸å®Œæˆï¼\n",
      "ğŸ“‹ å¯ç”¨è®Šæ•¸: mainland_filtering_results, authentic_mainland_data\n",
      "ğŸ¯ æœ€çµ‚è¼¸å‡ºç‚ºå¥å­ç´šåˆ¥çš„ç‰‡æ®µè³‡æ–™ (10-50å­—)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "CHECKPOINT_FILE = \"mainland_checkpoint.json\"\n",
    "PARTIAL_FILE    = \"mainland_partial.csv\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            ckpt = json.load(f)\n",
    "        print(f\"ğŸ”„ åµæ¸¬åˆ°é€²åº¦æª”ï¼Œå°‡å¾ index {ckpt['last_index'] + 1} ç¹¼çºŒ\")\n",
    "        return ckpt[\"last_index\"]\n",
    "    return -1\n",
    "\n",
    "def save_checkpoint(last_index):\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": last_index}, f)\n",
    "\n",
    "\n",
    "# ğŸ¯ æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸ç³»çµ± - ä½¿ç”¨ Ollama æ¨è«–ä¸¦å„²å­˜çµæœ\n",
    "print(\"ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\")\n",
    "\n",
    "# å®šç¾©å¤§é™¸ç‰¹æœ‰è©å½™åº«\n",
    "mainland_terms = {\n",
    "    \"è¨ˆç®—æ©Ÿ\": [\"é›»è…¦\"], \"è»Ÿä»¶\": [\"è»Ÿé«”\"], \"ç¡¬ä»¶\": [\"ç¡¬é«”\"], \"ç¶²çµ¡\": [\"ç¶²è·¯\"], \n",
    "    \"æ•¸æ“š\": [\"è³‡æ–™\"], \"ç¨‹åº\": [\"ç¨‹å¼\"], \"ä¿¡æ¯\": [\"è³‡è¨Š\"], \"å‡ºç§Ÿè»Š\": [\"è¨ˆç¨‹è»Š\"],\n",
    "    \"å…¬äº¤è»Š\": [\"å…¬è»Š\"], \"åœ°éµ\": [\"æ·é‹\"], \"è³ªé‡\": [\"å“è³ª\"], \"æœå‹™å“¡\": [\"æœå‹™ç”Ÿ\"],\n",
    "    \"åœŸè±†\": [\"é¦¬éˆ´è–¯\"], \"è¥¿ç´…æŸ¿\": [\"ç•ªèŒ„\"], \"æå®š\": [\"å®Œæˆ\"], \"æŒº\": [\"å¾ˆ\"],\n",
    "    \"å’‹\": [\"æ€éº¼\"], \"å•¥\": [\"ä»€éº¼\"], \"å¾®ä¿¡\": [\"\"], \"æ”¯ä»˜å¯¶\": [\"\"], \"æ·˜å¯¶\": [\"\"]\n",
    "}\n",
    "\n",
    "# å¤§é™¸èªæ³•æ¨¡å¼\n",
    "mainland_patterns = [r\"æŒº.*çš„\", r\"è ».*çš„\", r\".*å¾—å¾ˆ\", r\"å’‹.*\", r\"å•¥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"å¿«é€Ÿç‰¹å¾µåˆ†æ\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "\n",
    "async def mainland_score_api_async(text, session, model_endpoint, api_key, model_name):\n",
    "    \"\"\"ä½¿ç”¨ä½ æä¾›çš„ API éåŒæ­¥æ¨è«–å¤§é™¸ç”¨èªåˆ†æ•¸\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ä½ æ˜¯èªè¨€æª¢æ¸¬å·¥å…·ï¼Œè«‹å°ˆæ¥­çš„é‡å°ä¸‹åˆ—æ–‡æœ¬æŒ‰äº”é …æ¨™æº–æ‰“åˆ†ï¼Œæ¯é …ç‚º 0 æˆ– 1ï¼Œç¸½åˆ†ç‚º 0~5ã€‚\n",
    "\n",
    "è©•åˆ†æ¨™æº–ï¼š\n",
    "1. å¤§é™¸ç‰¹æœ‰è©å½™ï¼šè¨ˆç®—æ©Ÿã€è»Ÿä»¶ã€å‡ºç§Ÿè»Šã€åœ°éµç­‰\n",
    "2. å¤§é™¸èªæ³•ç¿’æ…£ï¼šæŒº...çš„ã€è »...çš„ã€å’‹æ¨£ç­‰  \n",
    "3. å¤§é™¸å£èªè¡¨é”ï¼šæå®šã€æ•´ã€å¼„ç­‰\n",
    "4. é¿å…ç¹é«”ç”¨èªï¼šä¸å«é›»è…¦ã€è»Ÿé«”ã€è³‡æ–™ç­‰\n",
    "5. æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦ï¼šç¶œåˆè©•ä¼°\n",
    "\n",
    "ä¸‹é¢æ˜¯è¦åˆ¤æ–·çš„æ–‡æœ¬\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è«‹æŒ‰æ ¼å¼å›ç­”ï¼š\n",
    "å¤§é™¸ç‰¹æœ‰è©å½™:0\n",
    "å¤§é™¸èªæ³•ç¿’æ…£:0\n",
    "å¤§é™¸å£èªè¡¨é”:0\n",
    "é¿å…ç¹é«”ç”¨èª:1\n",
    "æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦:0\n",
    "ç¸½åˆ†:1\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with session.post(model_endpoint, headers=headers, json=payload, timeout=60) as response:\n",
    "            data = await response.json()\n",
    "            reply = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", None)\n",
    "            return reply\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "def parse_scores(reply):\n",
    "    if not reply or not isinstance(reply, str):\n",
    "        # API æ²’å›æ±è¥¿ï¼Œç›´æ¥å›é è¨­åˆ†æ•¸\n",
    "        return {\n",
    "            \"å¤§é™¸ç‰¹æœ‰è©å½™\": 0,\n",
    "            \"å¤§é™¸èªæ³•ç¿’æ…£\": 0,\n",
    "            \"å¤§é™¸å£èªè¡¨é”\": 0,\n",
    "            \"é¿å…ç¹é«”ç”¨èª\": 0,\n",
    "            \"æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦\": 0,\n",
    "            \"ç¸½åˆ†\": 0\n",
    "        }\n",
    "\n",
    "    categories = ['å¤§é™¸ç‰¹æœ‰è©å½™', 'å¤§é™¸èªæ³•ç¿’æ…£', 'å¤§é™¸å£èªè¡¨é”', 'é¿å…ç¹é«”ç”¨èª', 'æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦']\n",
    "    scores = {}\n",
    "    for cat in categories:\n",
    "        match = re.search(fr\"{cat}\\s*[:ï¼š]\\s*(\\d)\", reply)\n",
    "        if match:\n",
    "            scores[cat] = int(match.group(1))\n",
    "        else:\n",
    "            scores[cat] = 0  # æ‰¾ä¸åˆ°å°±è£œ 0\n",
    "    scores['ç¸½åˆ†'] = sum(scores.values())\n",
    "    return scores\n",
    "\n",
    "async def process_dataset_async_batched(df, model_endpoint, api_key, model_name=\"gemma-3-27b-it\",\n",
    "                                        text_col='text', sample_size=200, threshold=3, batch_size=20):\n",
    "    print(f\"ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š{len(df)} ç­†\")\n",
    "    \"\"\"\n",
    "    åŠ å…¥æ–·é»çºŒå­˜çš„éåŒæ­¥æ‰¹æ¬¡è™•ç†\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š{len(df)} ç­†\")\n",
    "    # ğŸ‘‰ å¦‚æœä½ æƒ³è™•ç†æ•´å€‹ DataFrameï¼Œå°±ä¸è¦ sample\n",
    "    if sample_size:\n",
    "        df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "    last_index = load_checkpoint()\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "\n",
    "    # å¦‚æœæœ‰éƒ¨åˆ†çµæœï¼Œå…ˆè®€å–\n",
    "    if os.path.exists(PARTIAL_FILE):\n",
    "        partial_df = pd.read_csv(PARTIAL_FILE)\n",
    "\n",
    "        # ğŸ”‘ åŠ é€™æ®µï¼šæŠŠ CSV ä¸­å­˜æˆå­—ä¸²çš„ dict è½‰å›çœŸæ­£çš„ dict\n",
    "        for col in [\"features\", \"scores\"]:\n",
    "            if col in partial_df.columns:\n",
    "                partial_df[col] = partial_df[col].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "                )\n",
    "\n",
    "        results = partial_df.to_dict(\"records\")\n",
    "        print(f\"ğŸ” å·²è®€å–éƒ¨åˆ†è™•ç†çµæœï¼š{len(results)} ç­†\")\n",
    "        done_indices = set(partial_df[\"index\"].tolist())\n",
    "    else:\n",
    "        done_indices = set()\n",
    "\n",
    "    texts = df[text_col].tolist()\n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for batch_start in tqdm(range(0, len(texts), batch_size), desc=\"éåŒæ­¥æ‰¹æ¬¡æ¨è«–\"):\n",
    "            batch_texts = texts[batch_start:batch_start+batch_size]\n",
    "            batch_indices = indices[batch_start:batch_start+batch_size]\n",
    "\n",
    "            # è·³éå·²è™•ç†çš„ç´¢å¼•\n",
    "            filtered_batch = [\n",
    "                (idx, text) for idx, text in zip(batch_indices, batch_texts)\n",
    "                if idx not in done_indices and idx > last_index\n",
    "            ]\n",
    "            if not filtered_batch:\n",
    "                continue\n",
    "\n",
    "            tasks = [\n",
    "                mainland_score_api_async(text, session, model_endpoint, api_key, model_name)\n",
    "                for text in batch_texts\n",
    "            ]\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "\n",
    "            for i, response in enumerate(responses):\n",
    "                text = batch_texts[i]\n",
    "                idx = batch_indices[i]\n",
    "                features = analyze_features(text)\n",
    "                scores = parse_scores(response)\n",
    "\n",
    "                result = {\n",
    "                    'index': idx,\n",
    "                    'text': text,\n",
    "                    'text_length': len(text),\n",
    "                    'features': features,\n",
    "                    'scores': scores,\n",
    "                    'response': response,\n",
    "                    'success': scores is not None\n",
    "                }\n",
    "\n",
    "                if scores and scores.get(\"ç¸½åˆ†\", 0) >= threshold:\n",
    "                    result['category'] = \"çœŸæ­£å¤§é™¸ç”¨èª\"\n",
    "                    authentic_texts.append(result)\n",
    "                else:\n",
    "                    result['category'] = \"é€šç”¨ç°¡é«”ä¸­æ–‡\"\n",
    "                    generic_texts.append(result)\n",
    "\n",
    "                results.append(result)\n",
    "                done_indices.add(idx)\n",
    "            \n",
    "            pd.DataFrame(results).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "            save_checkpoint(max(done_indices))\n",
    "\n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"å„²å­˜ç¯©é¸çµæœ - æ”¯æ´åˆ‡åˆ†è³‡æ–™æ ¼å¼\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. å®Œæ•´çµæœ\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "\n",
    "        \n",
    "        # æ·»åŠ åˆ‡åˆ†ç›¸é—œæ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "        original_row = available_data.iloc[r['index']]\n",
    "        for col in [\n",
    "            'source_type','source','fragment_length','augmentation_method',\n",
    "            'original_idx','fragment_index','fragment_start','fragment_end'\n",
    "        ]:\n",
    "            if col in original_row:\n",
    "                row[col] = original_row[col]\n",
    "\n",
    "        if 'source_type' in original_row:\n",
    "            row['source_type'] = original_row['source_type']\n",
    "        if 'source' in original_row:\n",
    "            row['source'] = original_row['source']\n",
    "        if 'fragment_length' in original_row:\n",
    "            row['fragment_length'] = original_row['fragment_length']\n",
    "        if 'augmentation_method' in original_row:\n",
    "            row['augmentation_method'] = original_row['augmentation_method']\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. é«˜è³ªé‡å¤§é™¸ç”¨èªæ•¸æ“šï¼ˆåˆ‡åˆ†æ ¼å¼ï¼‰\n",
    "    if authentic_texts:\n",
    "        authentic_data = []\n",
    "        for r in authentic_texts:\n",
    "            original_row = available_data.iloc[r['index']]\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['ç¸½åˆ†'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length']\n",
    "            }\n",
    "            \n",
    "            # ä¿ç•™åˆ‡åˆ†ç›¸é—œæ¬„ä½\n",
    "            if 'source_type' in original_row:\n",
    "                auth_row['source_type'] = original_row['source_type']\n",
    "            if 'source' in original_row:\n",
    "                auth_row['source'] = original_row['source']\n",
    "            if 'fragment_length' in original_row:\n",
    "                auth_row['fragment_length'] = original_row['fragment_length']\n",
    "            if 'augmentation_method' in original_row:\n",
    "                auth_row['augmentation_method'] = original_row['augmentation_method']\n",
    "            if 'original_idx' in original_row:\n",
    "                auth_row['original_idx'] = original_row['original_idx']\n",
    "            if 'fragment_index' in original_row:\n",
    "                auth_row['fragment_index'] = original_row['fragment_index']\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ å„²å­˜å®Œæˆ:\")\n",
    "        print(f\"  ğŸ“„ å®Œæ•´çµæœ: {full_file}\")\n",
    "        print(f\"  âœ… é«˜è³ªé‡å¥å­ç‰‡æ®µæ•¸æ“š: {auth_csv}\")\n",
    "        print(f\"  ğŸ“‹ JSONæ ¼å¼: {auth_json}\")\n",
    "        \n",
    "        # é¡¯ç¤ºåˆ‡åˆ†è³‡æ–™çµ±è¨ˆ\n",
    "        if 'source' in auth_df.columns:\n",
    "            print(f\"\\nğŸ“Š é«˜è³ªé‡æ•¸æ“šä¾†æºåˆ†å¸ƒ:\")\n",
    "            print(auth_df['source'].value_counts())\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# ä¸»è¦åŸ·è¡Œæµç¨‹\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æª¢æŸ¥å¯ç”¨è³‡æ–™é›† (å„ªå…ˆä½¿ç”¨æœ€çµ‚åˆ‡åˆ†å¥å­ç‰‡æ®µè³‡æ–™é›†)\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'final_split_augmented_df' in locals() and final_split_augmented_df is not None:\n",
    "    available_data = final_split_augmented_df\n",
    "    source_name = \"æœ€çµ‚å¥å­ç‰‡æ®µæ“´å¢è³‡æ–™é›†\"\n",
    "elif 'split_dataset_df' in locals() and split_dataset_df is not None:\n",
    "    available_data = split_dataset_df\n",
    "    source_name = \"å¥å­ç‰‡æ®µè³‡æ–™é›†\"\n",
    "elif 'optimized_augmented_df' in locals() and optimized_augmented_df is not None:\n",
    "    available_data = optimized_augmented_df\n",
    "    source_name = \"å„ªåŒ–æ“´å¢è³‡æ–™é›†\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"åŸå§‹è³‡æ–™é›†\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"âœ… ä½¿ç”¨ {source_name}ï¼Œå…± {len(available_data)} ç­†è¨˜éŒ„\")\n",
    "    \n",
    "    # åŸ·è¡Œç¯©é¸ï¼ˆå¯èª¿æ•´åƒæ•¸ï¼‰\n",
    "    MODEL_API_ENDPOINT = \"https://labor-openwebui.dgx-coolify.apmic.ai/api/chat/completions\"\n",
    "    OPENWEBUI_API_KEY = API_KEY\n",
    "    MODEL_NAME = \"gemma-3-27b-it\"\n",
    "    SAMPLE_SIZE = 100\n",
    "    THRESHOLD = 3\n",
    "    BATCH_SIZE = 20 \n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹éåŒæ­¥æ‰¹æ¬¡è™•ç†ï¼Œæ¯æ‰¹ {BATCH_SIZE} ç­†...\")\n",
    "\n",
    "    # â—â—â— é€™è£¡ä¸è¦ç”¨ asyncio.run()ï¼Œç›´æ¥ await\n",
    "    results, authentic_results, generic_results = await process_dataset_async_batched(\n",
    "        df=available_data,\n",
    "        model_endpoint=MODEL_API_ENDPOINT,\n",
    "        api_key=OPENWEBUI_API_KEY,\n",
    "        model_name=MODEL_NAME,\n",
    "        text_col=text_column,\n",
    "        sample_size=None,\n",
    "        threshold=THRESHOLD,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆçµæœ\n",
    "    print(f\"\\nğŸ“Š ç¯©é¸çµæœçµ±è¨ˆ:\")\n",
    "    print(f\"  âœ… çœŸæ­£å¤§é™¸ç”¨èª: {len(authentic_results)} ç­†\")\n",
    "    print(f\"  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: {len(generic_results)} ç­†\")\n",
    "    print(f\"  ğŸ“ˆ ç¯©é¸ç‡: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # é¡¯ç¤ºç¯„ä¾‹\n",
    "    if authentic_results:\n",
    "        print(f\"\\nğŸ“ é«˜è³ªé‡å¤§é™¸ç”¨èªç¯„ä¾‹:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (å¾—åˆ†:{r['scores']['ç¸½åˆ†']}) {preview}\")\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    print(f\"\\nğŸ’¾ å„²å­˜çµæœ...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\nğŸ‰ å¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“‹ å¯ç”¨è®Šæ•¸: mainland_filtering_results, authentic_mainland_data\")\n",
    "    print(f\"ğŸ¯ æœ€çµ‚è¼¸å‡ºç‚ºå¥å­ç´šåˆ¥çš„ç‰‡æ®µè³‡æ–™ (10-50å­—)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„è³‡æ–™é›†\")\n",
    "    print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™è¼‰å…¥ã€æ–‡æœ¬åˆ‡åˆ†æˆ–æ“´å¢æ­¥é©Ÿ\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (housing)",
   "language": "python",
   "name": "housing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
