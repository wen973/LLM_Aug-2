{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import glob\n",
    "import ast "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 資料載入選項 ===\n",
      "使用本地檔案: False\n",
      "使用串流模式: False\n",
      "下載完整資料集: True\n",
      "\n",
      "⬇️ 下載完整資料集...\n",
      "警告：這將下載 13.7GB 的資料，可能需要很長時間\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed5eb895244fd7bb9f4ef1ab596ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 資料讀取選項\n",
    "# 您可以選擇以下任一種方式來載入資料：\n",
    "\n",
    "# 選項 1: 從已儲存的本地檔案讀取 (推薦，速度快)\n",
    "use_local_files = False\n",
    "\n",
    "# 選項 2: 從 Hugging Face 直接串流載入 (需要網路連線)\n",
    "use_streaming = False\n",
    "\n",
    "# 選項 3: 下載完整資料集 (檔案很大，不推薦)\n",
    "use_full_download = True\n",
    "\n",
    "print(\"=== 資料載入選項 ===\")\n",
    "print(f\"使用本地檔案: {use_local_files}\")\n",
    "print(f\"使用串流模式: {use_streaming}\")\n",
    "print(f\"下載完整資料集: {use_full_download}\")\n",
    "\n",
    "# 資料載入\n",
    "if use_local_files:\n",
    "    print(\"\\n📁 從本地檔案讀取資料...\")\n",
    "    \n",
    "    # 檢查已儲存的檔案\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # 尋找可用的檔案\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"找到的檔案:\")\n",
    "        print(f\"  CSV 檔案: {len(csv_files)} 個\")\n",
    "        print(f\"  JSON 檔案: {len(json_files)} 個\")\n",
    "        print(f\"  Parquet 檔案: {len(parquet_files)} 個\")\n",
    "        \n",
    "        # 優先使用 Parquet 檔案 (最高效)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 Parquet 檔案: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # 其次使用 CSV 檔案\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 CSV 檔案: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # 最後使用 JSON 檔案\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 JSON 檔案: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 沒有找到已儲存的資料檔案\")\n",
    "            print(\"請先執行資料下載和儲存的程式碼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"❌ 找不到 saved_datasets 目錄\")\n",
    "        print(\"請先執行資料下載和儲存的程式碼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\n🌐 從 Hugging Face 串流載入資料...\")\n",
    "    \n",
    "    # 使用串流模式載入資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # 設定要載入的樣本數量 - 減少到100筆用於演示\n",
    "    num_samples = 1000\n",
    "    print(f\"載入前 {num_samples} 筆資料...\")\n",
    "    \n",
    "    # 收集資料\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  已載入 {i + 1} 筆資料...\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\n⬇️ 下載完整資料集...\")\n",
    "    print(\"警告：這將下載 13.7GB 的資料，可能需要很長時間\")\n",
    "    \n",
    "    # 下載完整資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有選擇任何資料載入選項\")\n",
    "    df = None\n",
    "\n",
    "# 顯示資料資訊\n",
    "if df is not None:\n",
    "    print(f\"\\n✅ 資料載入成功！\")\n",
    "    print(f\"📊 資料形狀: {df.shape}\")\n",
    "    print(f\"📋 欄位名稱: {list(df.columns)}\")\n",
    "    \n",
    "    # 顯示基本統計\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\n📈 文本長度統計:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # 顯示前幾筆資料範例\n",
    "        print(f\"\\n📝 前 3 筆資料範例:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # 顯示前100個字符\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"範例 {i+1} ({len(text)} 字符): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\")\n",
    "else:\n",
    "    print(\"\\n❌ 資料載入失敗，請檢查設定並重新執行\")\n",
    "\n",
    "# 儲存到全域變數供後續使用\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = \"saved_datasets\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(save_dir, \"test_cluecorpus.csv\"), index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# dataset_df = pd.read_csv(\"test_cluecorpus.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc60868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\n",
    "    \"test_cluecorpus.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    nrows=1000   # 只讀取前 1000 筆\n",
    ")\n",
    "\n",
    "print(len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633432",
   "metadata": {},
   "source": [
    "## 📝 文本切分處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c683f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔪 啟動文本切分處理 (含斷點續存)...\n",
      "\n",
      "⚙️ 切分參數:\n",
      "  最小片段長度: 30 字\n",
      "  最大片段長度: 250 字\n",
      "  切分模式: 句子級別 + 斷點續存\n",
      "📊 開始處理文本切分...\n",
      "  原始資料筆數: 1000\n",
      "  句子片段長度範圍: 30-250 字\n",
      "🔁 已讀取部分處理結果：32800 筆\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "切分進度: 100%|██████████| 1000/1000 [02:02<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 全部處理完成，已刪除斷點檔\n",
      "\n",
      "✅ 文本切分完成！\n",
      "  處理文本數: 794\n",
      "  生成句子片段數: 36670\n",
      "  平均每文本片段數: 46.2\n",
      "\n",
      "📏 片段長度統計:\n",
      "  平均長度: 62.0 字\n",
      "  最短片段: 30 字\n",
      "  最長片段: 1049 字\n",
      "  中位數長度: 51.0 字\n",
      "\n",
      "💾 完成輸出:\n",
      "  📄 CSV: split_datasets/sentence_fragments_20250911_201937.csv\n",
      "  📋 JSON: split_datasets/sentence_fragments_20250911_201937.json\n",
      "  📦 Parquet: split_datasets/sentence_fragments_20250911_201937.parquet\n",
      "\n",
      "🎯 變數名稱: split_dataset_df\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"🔪 啟動文本切分處理 (含斷點續存)...\")\n",
    "\n",
    "# =========================================================\n",
    "# 句子切分核心函數\n",
    "# =========================================================\n",
    "def split_text_to_sentences(text, min_length=30, max_length=250):\n",
    "    sentence_separators = ['。', '！', '？', '；', '…']  # 強分隔\n",
    "    phrase_separators = ['，', '、', '：', '；']       # 弱分隔\n",
    "\n",
    "    sentences, current_sentence = [], \"\"\n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in sentence_separators:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "\n",
    "    fragments = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) < min_length:\n",
    "            continue\n",
    "        if len(sentence) <= max_length:\n",
    "            fragments.append(sentence)\n",
    "        else:\n",
    "            # 長句再細切\n",
    "            parts, current_part = [], \"\"\n",
    "            for char in sentence:\n",
    "                current_part += char\n",
    "                if char in phrase_separators:\n",
    "                    if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                        parts.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "            if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                parts.append(current_part.strip())\n",
    "\n",
    "            merged_parts, temp_part = [], \"\"\n",
    "            for part in parts:\n",
    "                if len(temp_part + part) <= max_length:\n",
    "                    temp_part = temp_part + part if temp_part else part\n",
    "                else:\n",
    "                    if temp_part and len(temp_part) >= min_length:\n",
    "                        merged_parts.append(temp_part)\n",
    "                    temp_part = part\n",
    "            if temp_part and len(temp_part) >= min_length:\n",
    "                merged_parts.append(temp_part)\n",
    "\n",
    "            # 無法細切就硬切\n",
    "            if not merged_parts and len(sentence) >= min_length:\n",
    "                for i in range(0, len(sentence), max_length):\n",
    "                    fragment = sentence[i:i + max_length]\n",
    "                    if len(fragment) >= min_length:\n",
    "                        merged_parts.append(fragment)\n",
    "\n",
    "            fragments.extend(merged_parts)\n",
    "    return fragments\n",
    "\n",
    "def split_text_by_punctuation(text, min_length=30, max_length=250):\n",
    "    return split_text_to_sentences(text, min_length, max_length)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 斷點續存輔助函數\n",
    "# =========================================================\n",
    "CHECKPOINT_FILE = \"split_checkpoint.json\"\n",
    "PARTIAL_FILE    = \"split_partial.csv\"\n",
    "\n",
    "def save_checkpoint(last_index):\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": last_index}, f)\n",
    "    # print(f\"💾 已儲存進度：index {last_index}\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            ckpt = json.load(f)\n",
    "        print(f\"🔄 偵測到進度檔，將從 index {ckpt['last_index'] + 1} 繼續\")\n",
    "        return ckpt[\"last_index\"]\n",
    "    return -1\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 主處理流程（已加入斷點續存）\n",
    "# =========================================================\n",
    "def process_text_splitting(df, text_column='text', min_length=30, max_length=250):\n",
    "    print(f\"📊 開始處理文本切分...\")\n",
    "    print(f\"  原始資料筆數: {len(df)}\")\n",
    "    print(f\"  句子片段長度範圍: {min_length}-{max_length} 字\")\n",
    "\n",
    "    last_index = load_checkpoint()\n",
    "    split_data = []\n",
    "\n",
    "    # 讀取部分輸出以便續存\n",
    "    if os.path.exists(PARTIAL_FILE):\n",
    "        split_data = pd.read_csv(PARTIAL_FILE).to_dict(\"records\")\n",
    "        print(f\"🔁 已讀取部分處理結果：{len(split_data)} 筆\")\n",
    "\n",
    "    total_fragments, processed_texts = len(split_data), 0\n",
    "\n",
    "    CHECKPOINT_INTERVAL = 1000\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"切分進度\"):\n",
    "        if idx <= last_index:\n",
    "            continue  # 跳過已處理\n",
    "\n",
    "        if idx % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(idx)\n",
    "            pd.DataFrame(split_data).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        original_text = row[text_column]\n",
    "        if pd.isna(original_text):\n",
    "            continue          # ⬅️ 建議直接跳過\n",
    "        original_text = str(original_text)\n",
    "        \n",
    "        if len(original_text) < min_length:\n",
    "            continue\n",
    "\n",
    "        search_start_pos = 0\n",
    "        fragments = split_text_to_sentences(original_text, min_length, max_length)\n",
    "\n",
    "        for frag_idx, fragment in enumerate(fragments):\n",
    "            start_pos = original_text.find(fragment, search_start_pos)\n",
    "            end_pos   = start_pos + len(fragment) if start_pos != -1 else -1\n",
    "            if start_pos != -1:\n",
    "                search_start_pos = end_pos\n",
    "\n",
    "            new_row = row.copy()\n",
    "            new_row[text_column] = fragment\n",
    "            new_row['original_index'] = idx\n",
    "            new_row['fragment_index'] = frag_idx\n",
    "            new_row['original_text_length'] = len(original_text)\n",
    "            new_row['fragment_length'] = len(fragment)\n",
    "            new_row['source_type'] = 'sentence_fragment'\n",
    "            new_row['fragment_start'] = start_pos\n",
    "            new_row['fragment_end'] = end_pos\n",
    "            split_data.append(new_row)\n",
    "            total_fragments += 1\n",
    "\n",
    "        processed_texts += 1\n",
    "\n",
    "        # ✅ 每處理一筆就存檔\n",
    "        save_checkpoint(idx)\n",
    "        pd.DataFrame(split_data).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 全部完成後刪除 checkpoint\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "        print(\"🎉 全部處理完成，已刪除斷點檔\")\n",
    "\n",
    "    split_df = pd.DataFrame(split_data)\n",
    "\n",
    "    print(f\"\\n✅ 文本切分完成！\")\n",
    "    print(f\"  處理文本數: {processed_texts}\")\n",
    "    print(f\"  生成句子片段數: {total_fragments}\")\n",
    "    if processed_texts:\n",
    "        print(f\"  平均每文本片段數: {total_fragments/processed_texts:.1f}\")\n",
    "\n",
    "    if not split_df.empty:\n",
    "        length_stats = split_df['fragment_length'].describe()\n",
    "        print(f\"\\n📏 片段長度統計:\")\n",
    "        print(f\"  平均長度: {length_stats['mean']:.1f} 字\")\n",
    "        print(f\"  最短片段: {length_stats['min']:.0f} 字\")\n",
    "        print(f\"  最長片段: {length_stats['max']:.0f} 字\")\n",
    "        print(f\"  中位數長度: {length_stats['50%']:.1f} 字\")\n",
    "\n",
    "    return split_df\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 執行流程示範\n",
    "# =========================================================\n",
    "if 'dataset_df' in globals() and dataset_df is not None:\n",
    "    MIN_FRAGMENT_LENGTH = 30\n",
    "    MAX_FRAGMENT_LENGTH = 250\n",
    "\n",
    "    print(f\"\\n⚙️ 切分參數:\")\n",
    "    print(f\"  最小片段長度: {MIN_FRAGMENT_LENGTH} 字\")\n",
    "    print(f\"  最大片段長度: {MAX_FRAGMENT_LENGTH} 字\")\n",
    "    print(f\"  切分模式: 句子級別 + 斷點續存\")\n",
    "\n",
    "    split_dataset_df = process_text_splitting(\n",
    "        df=dataset_df,\n",
    "        text_column='text',\n",
    "        min_length=MIN_FRAGMENT_LENGTH,\n",
    "        max_length=MAX_FRAGMENT_LENGTH\n",
    "    )\n",
    "\n",
    "    if not split_dataset_df.empty:\n",
    "        # 另存最終檔案（附時間戳）\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        split_dir = \"split_datasets\"\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        base_filename = f\"{split_dir}/sentence_fragments_{timestamp}\"\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "\n",
    "        int_cols = [\n",
    "        \"original_index\", \"fragment_index\",\n",
    "        \"original_text_length\", \"fragment_length\",\n",
    "        \"fragment_start\", \"fragment_end\"\n",
    "        ]\n",
    "        for c in int_cols:\n",
    "            if c in split_dataset_df.columns:\n",
    "                split_dataset_df[c] = (\n",
    "                    pd.to_numeric(split_dataset_df[c], errors=\"coerce\")\n",
    "                    .astype(\"Int64\")\n",
    "                )\n",
    "\n",
    "        split_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        split_dataset_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        split_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "\n",
    "        print(f\"\\n💾 完成輸出:\")\n",
    "        print(f\"  📄 CSV: {csv_filename}\")\n",
    "        print(f\"  📋 JSON: {json_filename}\")\n",
    "        print(f\"  📦 Parquet: {parquet_filename}\")\n",
    "\n",
    "        globals()['split_dataset_df'] = split_dataset_df\n",
    "        print(f\"\\n🎯 變數名稱: split_dataset_df\")\n",
    "    else:\n",
    "        print(\"❌ 沒有產生有效片段\")\n",
    "        split_dataset_df = None\n",
    "else:\n",
    "    print(\"❌ 沒有找到資料集，請先執行 Get Data\")\n",
    "    split_dataset_df = None\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7ae23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>original_index</th>\n",
       "      <th>fragment_index</th>\n",
       "      <th>original_text_length</th>\n",
       "      <th>fragment_length</th>\n",
       "      <th>source_type</th>\n",
       "      <th>fragment_start</th>\n",
       "      <th>fragment_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>兰州公交集团:明起至8月底三条公交线暂不经过靖远路站原标题：明起至8月底三条公交线暂不经过靖...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>155</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>据悉，35路公交线由兰州西客站开往美伦广场时，兰州西客站至白塔山公园线路不变，经北滨河路九州...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "      <td>71</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>155</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>由美伦广场开往兰州西客站时，美伦广场至庙滩子线路不变，左转弯进入九州大道至北滨河路市二医院恢...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>545</td>\n",
       "      <td>53</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>226</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81路公交线路由市二医院开往省军干所时，经北滨河路九州大道南口左转弯进入九州大道至庙滩子恢复...</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>545</td>\n",
       "      <td>52</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>279</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>由省军干所开往市二医院时，省军干所至庙滩子线路不变，左转弯进入九州大道至市二医院。</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>545</td>\n",
       "      <td>41</td>\n",
       "      <td>sentence_fragment</td>\n",
       "      <td>331</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_length  \\\n",
       "0  兰州公交集团:明起至8月底三条公交线暂不经过靖远路站原标题：明起至8月底三条公交线暂不经过靖...          545   \n",
       "1  据悉，35路公交线由兰州西客站开往美伦广场时，兰州西客站至白塔山公园线路不变，经北滨河路九州...          545   \n",
       "2  由美伦广场开往兰州西客站时，美伦广场至庙滩子线路不变，左转弯进入九州大道至北滨河路市二医院恢...          545   \n",
       "3  81路公交线路由市二医院开往省军干所时，经北滨河路九州大道南口左转弯进入九州大道至庙滩子恢复...          545   \n",
       "4          由省军干所开往市二医院时，省军干所至庙滩子线路不变，左转弯进入九州大道至市二医院。          545   \n",
       "\n",
       "   original_index  fragment_index  original_text_length  fragment_length  \\\n",
       "0               3               0                   545              155   \n",
       "1               3               1                   545               71   \n",
       "2               3               2                   545               53   \n",
       "3               3               3                   545               52   \n",
       "4               3               4                   545               41   \n",
       "\n",
       "         source_type  fragment_start  fragment_end  \n",
       "0  sentence_fragment               0           155  \n",
       "1  sentence_fragment             155           226  \n",
       "2  sentence_fragment             226           279  \n",
       "3  sentence_fragment             279           331  \n",
       "4  sentence_fragment             331           372  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " split_dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa141b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== CONFIG ====\n",
    "# MODEL_API_ENDPOINT = \"https://labor- openwebui.dgx-coolify.apmic.ai/api/\"\n",
    "# API_KEY  = \"請填入自己的API KEY\"\n",
    "# MODEL_NAME  = \"gemma-3-27b-it\"\n",
    "# BATCH_SIZE  = 20          # 視API能力可調\n",
    "# THRESHOLD   = 3\n",
    "# OUTPUT_DIR  = \"./results\" # 儲存結果的資料夾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY =\"sk-b56c488f33b94df297a6314bd037b805\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41859ef8",
   "metadata": {},
   "source": [
    "## gemma- 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35f7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動最終版大陸用語識別系統...\n",
      "============================================================\n",
      "✅ 使用 句子片段資料集，共 36670 筆記錄\n",
      "\n",
      "🚀 開始非同步批次處理，每批 20 筆...\n",
      "📊 處理資料集：36670 筆\n",
      "📊 處理資料集：36670 筆\n",
      "🔄 偵測到進度檔，將從 index 35803 繼續\n",
      "🔁 已讀取部分處理結果：100 筆\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "非同步批次推論: 100%|██████████| 1834/1834 [09:24<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 篩選結果統計:\n",
      "  ✅ 真正大陸用語: 109 筆\n",
      "  🗑️ 通用簡體中文: 761 筆\n",
      "  📈 篩選率: 11.2%\n",
      "\n",
      "📝 高質量大陸用語範例:\n",
      "  1. (得分:3) 目前从国内来看，instagram略胜一筹，因为国内能买的上iphone的谁不自恋？\n",
      "  2. (得分:4) 鱼子酱咸鱼什么的：冲着鱼子酱点的， 结果真得不咋地，哎，看来我这辈子也就是个穷人的命啦。\n",
      "  3. (得分:3) 小黄鱼烧豆腐：很嫩很嫩的，鱼也嫩，豆腐也嫩，就是一碰就碎，有点不知道怎么下手。\n",
      "\n",
      "💾 儲存結果...\n",
      "💾 儲存完成:\n",
      "  📄 完整結果: mainland_filtering_complete_20250911_203642.csv\n",
      "  ✅ 高質量句子片段數據: authentic_mainland_texts_20250911_203642.csv\n",
      "  📋 JSON格式: authentic_mainland_texts_20250911_203642.json\n",
      "\n",
      "🎉 大陸用語識別與篩選完成！\n",
      "📋 可用變數: mainland_filtering_results, authentic_mainland_data\n",
      "🎯 最終輸出為句子級別的片段資料 (10-50字)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "CHECKPOINT_FILE = \"mainland_checkpoint.json\"\n",
    "PARTIAL_FILE    = \"mainland_partial.csv\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            ckpt = json.load(f)\n",
    "        print(f\"🔄 偵測到進度檔，將從 index {ckpt['last_index'] + 1} 繼續\")\n",
    "        return ckpt[\"last_index\"]\n",
    "    return -1\n",
    "\n",
    "def save_checkpoint(last_index):\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": last_index}, f)\n",
    "\n",
    "\n",
    "# 🎯 最終版大陸用語識別與篩選系統 - 使用 Ollama 推論並儲存結果\n",
    "print(\"🚀 啟動最終版大陸用語識別系統...\")\n",
    "\n",
    "# 定義大陸特有詞彙庫\n",
    "mainland_terms = {\n",
    "    \"計算機\": [\"電腦\"], \"軟件\": [\"軟體\"], \"硬件\": [\"硬體\"], \"網絡\": [\"網路\"], \n",
    "    \"數據\": [\"資料\"], \"程序\": [\"程式\"], \"信息\": [\"資訊\"], \"出租車\": [\"計程車\"],\n",
    "    \"公交車\": [\"公車\"], \"地鐵\": [\"捷運\"], \"質量\": [\"品質\"], \"服務員\": [\"服務生\"],\n",
    "    \"土豆\": [\"馬鈴薯\"], \"西紅柿\": [\"番茄\"], \"搞定\": [\"完成\"], \"挺\": [\"很\"],\n",
    "    \"咋\": [\"怎麼\"], \"啥\": [\"什麼\"], \"微信\": [\"\"], \"支付寶\": [\"\"], \"淘寶\": [\"\"]\n",
    "}\n",
    "\n",
    "# 大陸語法模式\n",
    "mainland_patterns = [r\"挺.*的\", r\"蠻.*的\", r\".*得很\", r\"咋.*\", r\"啥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"快速特徵分析\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "\n",
    "async def mainland_score_api_async(text, session, model_endpoint, api_key, model_name):\n",
    "    \"\"\"使用你提供的 API 非同步推論大陸用語分數\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    你是語言檢測工具，請專業的針對下列文本按五項標準打分，每項為 0 或 1，總分為 0~5。\n",
    "\n",
    "評分標準：\n",
    "1. 大陸特有詞彙：計算機、軟件、出租車、地鐵等\n",
    "2. 大陸語法習慣：挺...的、蠻...的、咋樣等  \n",
    "3. 大陸口語表達：搞定、整、弄等\n",
    "4. 避免繁體用語：不含電腦、軟體、資料等\n",
    "5. 整體大陸化程度：綜合評估\n",
    "\n",
    "下面是要判斷的文本\n",
    "文本：{text}\n",
    "\n",
    "請按格式回答：\n",
    "大陸特有詞彙:0\n",
    "大陸語法習慣:0\n",
    "大陸口語表達:0\n",
    "避免繁體用語:1\n",
    "整體大陸化程度:0\n",
    "總分:1\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with session.post(model_endpoint, headers=headers, json=payload, timeout=60) as response:\n",
    "            data = await response.json()\n",
    "            reply = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", None)\n",
    "            return reply\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "def parse_scores(reply):\n",
    "    if not reply or not isinstance(reply, str):\n",
    "        # API 沒回東西，直接回預設分數\n",
    "        return {\n",
    "            \"大陸特有詞彙\": 0,\n",
    "            \"大陸語法習慣\": 0,\n",
    "            \"大陸口語表達\": 0,\n",
    "            \"避免繁體用語\": 0,\n",
    "            \"整體大陸化程度\": 0,\n",
    "            \"總分\": 0\n",
    "        }\n",
    "\n",
    "    categories = ['大陸特有詞彙', '大陸語法習慣', '大陸口語表達', '避免繁體用語', '整體大陸化程度']\n",
    "    scores = {}\n",
    "    for cat in categories:\n",
    "        match = re.search(fr\"{cat}\\s*[:：]\\s*(\\d)\", reply)\n",
    "        if match:\n",
    "            scores[cat] = int(match.group(1))\n",
    "        else:\n",
    "            scores[cat] = 0  # 找不到就補 0\n",
    "    scores['總分'] = sum(scores.values())\n",
    "    return scores\n",
    "\n",
    "async def process_dataset_async_batched(df, model_endpoint, api_key, model_name=\"gemma-3-27b-it\",\n",
    "                                        text_col='text', sample_size=200, threshold=3, batch_size=20):\n",
    "    print(f\"📊 處理資料集：{len(df)} 筆\")\n",
    "    \"\"\"\n",
    "    加入斷點續存的非同步批次處理\n",
    "    \"\"\"\n",
    "    print(f\"📊 處理資料集：{len(df)} 筆\")\n",
    "    # 👉 如果你想處理整個 DataFrame，就不要 sample\n",
    "    if sample_size:\n",
    "        df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "    last_index = load_checkpoint()\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "\n",
    "    # 如果有部分結果，先讀取\n",
    "    if os.path.exists(PARTIAL_FILE):\n",
    "        partial_df = pd.read_csv(PARTIAL_FILE)\n",
    "\n",
    "        # 🔑 加這段：把 CSV 中存成字串的 dict 轉回真正的 dict\n",
    "        for col in [\"features\", \"scores\"]:\n",
    "            if col in partial_df.columns:\n",
    "                partial_df[col] = partial_df[col].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "                )\n",
    "\n",
    "        results = partial_df.to_dict(\"records\")\n",
    "        print(f\"🔁 已讀取部分處理結果：{len(results)} 筆\")\n",
    "        done_indices = set(partial_df[\"index\"].tolist())\n",
    "    else:\n",
    "        done_indices = set()\n",
    "\n",
    "    texts = df[text_col].tolist()\n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for batch_start in tqdm(range(0, len(texts), batch_size), desc=\"非同步批次推論\"):\n",
    "            batch_texts = texts[batch_start:batch_start+batch_size]\n",
    "            batch_indices = indices[batch_start:batch_start+batch_size]\n",
    "\n",
    "            # 跳過已處理的索引\n",
    "            filtered_batch = [\n",
    "                (idx, text) for idx, text in zip(batch_indices, batch_texts)\n",
    "                if idx not in done_indices and idx > last_index\n",
    "            ]\n",
    "            if not filtered_batch:\n",
    "                continue\n",
    "\n",
    "            tasks = [\n",
    "                mainland_score_api_async(text, session, model_endpoint, api_key, model_name)\n",
    "                for text in batch_texts\n",
    "            ]\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "\n",
    "            for i, response in enumerate(responses):\n",
    "                text = batch_texts[i]\n",
    "                idx = batch_indices[i]\n",
    "                features = analyze_features(text)\n",
    "                scores = parse_scores(response)\n",
    "\n",
    "                result = {\n",
    "                    'index': idx,\n",
    "                    'text': text,\n",
    "                    'text_length': len(text),\n",
    "                    'features': features,\n",
    "                    'scores': scores,\n",
    "                    'response': response,\n",
    "                    'success': scores is not None\n",
    "                }\n",
    "\n",
    "                if scores and scores.get(\"總分\", 0) >= threshold:\n",
    "                    result['category'] = \"真正大陸用語\"\n",
    "                    authentic_texts.append(result)\n",
    "                else:\n",
    "                    result['category'] = \"通用簡體中文\"\n",
    "                    generic_texts.append(result)\n",
    "\n",
    "                results.append(result)\n",
    "                done_indices.add(idx)\n",
    "            \n",
    "            pd.DataFrame(results).to_csv(PARTIAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "            save_checkpoint(max(done_indices))\n",
    "\n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"儲存篩選結果 - 支援切分資料格式\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 完整結果\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "\n",
    "        \n",
    "        # 添加切分相關欄位（如果存在）\n",
    "        original_row = available_data.iloc[r['index']]\n",
    "        for col in [\n",
    "            'source_type','source','fragment_length','augmentation_method',\n",
    "            'original_idx','fragment_index','fragment_start','fragment_end'\n",
    "        ]:\n",
    "            if col in original_row:\n",
    "                row[col] = original_row[col]\n",
    "\n",
    "        if 'source_type' in original_row:\n",
    "            row['source_type'] = original_row['source_type']\n",
    "        if 'source' in original_row:\n",
    "            row['source'] = original_row['source']\n",
    "        if 'fragment_length' in original_row:\n",
    "            row['fragment_length'] = original_row['fragment_length']\n",
    "        if 'augmentation_method' in original_row:\n",
    "            row['augmentation_method'] = original_row['augmentation_method']\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. 高質量大陸用語數據（切分格式）\n",
    "    if authentic_texts:\n",
    "        authentic_data = []\n",
    "        for r in authentic_texts:\n",
    "            original_row = available_data.iloc[r['index']]\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['總分'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length']\n",
    "            }\n",
    "            \n",
    "            # 保留切分相關欄位\n",
    "            if 'source_type' in original_row:\n",
    "                auth_row['source_type'] = original_row['source_type']\n",
    "            if 'source' in original_row:\n",
    "                auth_row['source'] = original_row['source']\n",
    "            if 'fragment_length' in original_row:\n",
    "                auth_row['fragment_length'] = original_row['fragment_length']\n",
    "            if 'augmentation_method' in original_row:\n",
    "                auth_row['augmentation_method'] = original_row['augmentation_method']\n",
    "            if 'original_idx' in original_row:\n",
    "                auth_row['original_idx'] = original_row['original_idx']\n",
    "            if 'fragment_index' in original_row:\n",
    "                auth_row['fragment_index'] = original_row['fragment_index']\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"💾 儲存完成:\")\n",
    "        print(f\"  📄 完整結果: {full_file}\")\n",
    "        print(f\"  ✅ 高質量句子片段數據: {auth_csv}\")\n",
    "        print(f\"  📋 JSON格式: {auth_json}\")\n",
    "        \n",
    "        # 顯示切分資料統計\n",
    "        if 'source' in auth_df.columns:\n",
    "            print(f\"\\n📊 高質量數據來源分布:\")\n",
    "            print(auth_df['source'].value_counts())\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# 主要執行流程\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 檢查可用資料集 (優先使用最終切分句子片段資料集)\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'final_split_augmented_df' in locals() and final_split_augmented_df is not None:\n",
    "    available_data = final_split_augmented_df\n",
    "    source_name = \"最終句子片段擴增資料集\"\n",
    "elif 'split_dataset_df' in locals() and split_dataset_df is not None:\n",
    "    available_data = split_dataset_df\n",
    "    source_name = \"句子片段資料集\"\n",
    "elif 'optimized_augmented_df' in locals() and optimized_augmented_df is not None:\n",
    "    available_data = optimized_augmented_df\n",
    "    source_name = \"優化擴增資料集\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"原始資料集\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"✅ 使用 {source_name}，共 {len(available_data)} 筆記錄\")\n",
    "    \n",
    "    # 執行篩選（可調整參數）\n",
    "    MODEL_API_ENDPOINT = \"https://labor-openwebui.dgx-coolify.apmic.ai/api/chat/completions\"\n",
    "    OPENWEBUI_API_KEY = API_KEY\n",
    "    MODEL_NAME = \"gemma-3-27b-it\"\n",
    "    SAMPLE_SIZE = 100\n",
    "    THRESHOLD = 3\n",
    "    BATCH_SIZE = 20 \n",
    "\n",
    "    print(f\"\\n🚀 開始非同步批次處理，每批 {BATCH_SIZE} 筆...\")\n",
    "\n",
    "    # ❗❗❗ 這裡不要用 asyncio.run()，直接 await\n",
    "    results, authentic_results, generic_results = await process_dataset_async_batched(\n",
    "        df=available_data,\n",
    "        model_endpoint=MODEL_API_ENDPOINT,\n",
    "        api_key=OPENWEBUI_API_KEY,\n",
    "        model_name=MODEL_NAME,\n",
    "        text_col=text_column,\n",
    "        sample_size=None,\n",
    "        threshold=THRESHOLD,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # 統計結果\n",
    "    print(f\"\\n📊 篩選結果統計:\")\n",
    "    print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "    print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "    print(f\"  📈 篩選率: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # 顯示範例\n",
    "    if authentic_results:\n",
    "        print(f\"\\n📝 高質量大陸用語範例:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (得分:{r['scores']['總分']}) {preview}\")\n",
    "    \n",
    "    # 儲存結果\n",
    "    print(f\"\\n💾 儲存結果...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # 設定全域變數\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\n🎉 大陸用語識別與篩選完成！\")\n",
    "    print(f\"📋 可用變數: mainland_filtering_results, authentic_mainland_data\")\n",
    "    print(f\"🎯 最終輸出為句子級別的片段資料 (10-50字)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 沒有找到可用的資料集\")\n",
    "    print(\"💡 請先執行前面的資料載入、文本切分或擴增步驟\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (housing)",
   "language": "python",
   "name": "housing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
